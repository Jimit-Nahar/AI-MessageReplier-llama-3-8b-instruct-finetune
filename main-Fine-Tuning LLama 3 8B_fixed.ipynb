{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"id": "2eSvM9zX_2d3"}, "outputs": [], "source": ["%%capture\n", "import torch\n", "major_version, minor_version = torch.cuda.get_device_capability()\n", "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n", "if major_version >= 8:\n", "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n", "else:\n", "    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n", "pass"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 333, "referenced_widgets": ["8a9e85e9e12d4bc28ee0342f52dbe5cf", "035fdd3dc1664b5f9905e5843cfc420c", "654f58364f3b4b1b832a3cd9d8bf59cb", "112b45dc53684b5aa1b61195bb3b896b", "60dc903696744a69b46ff01cc1c4c272", "ddbbdb24759f44b399a3a49b347f09a7", "b6b6f03ca609430db7b3db4df94aaf0d", "af7d632ff03a4a9a9484d208e340cdef", "5e508e5b9e6f46da8b65fe182337f3a8", "eb879f7027e748118c0c6a92d147d885", "ad8295896e6f41708d57be3ca185f186", "e5519478291a457896367758d4b71580", "dc20c9d24fc444b7b373479735ee9dfb", "b9b7afdf25474480b3f6369329da1f83", "413748da4d5047b3a24f90304bd2b566", "bff39681dd4242d6834061ca02782de6", "7f373acfcbfa4286b7af0b74ceb9753e", "4007eec8fcb04405917ebd744e7732bb", "01bcc0e5af3346f58c668dfd78ef8b90", "c0000fcf068144ca95508e2e6adee0a2", "725edcc6e5f54724bb7188547c2da57f", "31422c4367a449638d23f26df1cfc112", "1c96eeec92df46c0ba4abc7e8d8afd88", "bddb2cbd8e74414698ee106fece04cad", "c67c74a8d5ad45eaa822940001e477f4", "1b207fb1021a4060a0bfaca7a50c0cb8", "d8de165facb742e2924ea712e9a25d26", "184c73af3631498bbf027c99e2ed48c6", "3065755fc14143798fa864d5bd3a48d8", "9264cbc0724c40eaa7b088a4e670c2b9", "66b6c23e044b40ac99818e834d0a4b37", "fbc9c9ed40e04fe6a7abe66d28356882", "cf3e7e0dff414bcfa61715dada56beef", "49be3e9183574dc891e60543b67c19bb", "50c3e568ba68430aa564ea6f79ad57c8", "c0d3b5d8d5de486c8eb6186634010052", "0756bef87957453fa34009d7c8743068", "2738985413414ff792a747354ee62fc0", "c28ba6f67c564d0abfb334e2ed45b44f", "ea99beefd27a44799755885acf3d5be2", "c937e26f4f5d4ef8bfe827e79391ac03", "a481516a7eb24985a3fc47ddd710ba51", "4228ea5d24374dd3bbfd01d78a5f42b0", "68cfa9e5900845c08288b7f2b47b6693", "fa670e4da0e0474e863a1a15ff0c75a5", "415b1629fd634b7aa3321517d2fdf5ac", "2e82074f1d9b4d26b45cb7436dbc7e1a", "acb5aa92dcc441299961326c35bb8adf", "325ac183aa0044c1b0340d8ba85721af", "c334db3f9d0f41bdb7f8e04104b5d08d", "cb5af31969b141d18ab0421517c441f2", "af15a273a8cf4662ac3f49fbd172f4d9", "26d2738953aa43d08c9e00b0f7869745", "bef9671d0c204ba39a0e255e3cfa0551", "f9798d1492e34128b6afecd05df2ff82", "45c06b999ad24f1fa60fc5637f0f33b6", "7892ecb8c1194abbb1875aaae80ecfa9", "40e8e1fde851409eb9d43bf347f033cf", "6cc6d134b7b44edf8694ad938ba7be91", "f9fe4195d39e4d48a31765585b94afc6", "7d1a5c9203eb4c7286818edde90a4908", "e1c460589dd3464db285ce023eee990c", "847db033e344495b988ed05b417ec7d8", "d018a40da74b45a7a0950bab5e785685", "9e81aad2f70b42e797637c32f4b93a4a", "da3fdf1494394215ba45b32b0f07d4c7"]}, "id": "QmUBVEnvCDJv", "executionInfo": {"status": "ok", "timestamp": 1713733486123, "user_tz": -120, "elapsed": 95225, "user": {"displayName": "David Ondrej", "userId": "10143559659389860010"}}, "outputId": "4889726b-c306-4f96-d2a3-c1ec2f0caae5"}, "outputs": [{"output_type": "display_data", "data": {"text/plain": ["config.json:   0%|          | 0.00/1.14k [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "8a9e85e9e12d4bc28ee0342f52dbe5cf"}}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": ["==((====))==  Unsloth: Fast Llama patching release 2024.4\n", "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n", "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n", "\\        /    Bfloat16 = FALSE. Xformers = 0.0.25.post1. FA = False.\n", " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"]}, {"output_type": "display_data", "data": {"text/plain": ["model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "e5519478291a457896367758d4b71580"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["generation_config.json:   0%|          | 0.00/121 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "1c96eeec92df46c0ba4abc7e8d8afd88"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "49be3e9183574dc891e60543b67c19bb"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "fa670e4da0e0474e863a1a15ff0c75a5"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["special_tokens_map.json:   0%|          | 0.00/449 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "45c06b999ad24f1fa60fc5637f0f33b6"}}, "metadata": {}}, {"output_type": "stream", "name": "stderr", "text": ["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n", "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}], "source": ["from unsloth import FastLanguageModel\n", "import torch\n", "max_seq_length = 2048 # Choose any, Llama 3 is up to 8k\n", "dtype = None\n", "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n", "\n", "fourbit_models = [\n", "    \"unsloth/mistral-7b-bnb-4bit\",\n", "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n", "    \"unsloth/llama-2-7b-bnb-4bit\",\n", "    \"unsloth/gemma-7b-bnb-4bit\",\n", "    \"unsloth/gemma-7b-it-bnb-4bit\",\n", "    \"unsloth/gemma-2b-bnb-4bit\",\n", "    \"unsloth/gemma-2b-it-bnb-4bit\",\n", "    \"unsloth/llama-3-8b-bnb-4bit\",\n", "]\n", "\n", "model, tokenizer = FastLanguageModel.from_pretrained(\n", "    model_name = \"unsloth/llama-3-8b-bnb-4bit\", # Llama-3 70b also works (just change the model name)\n", "    max_seq_length = max_seq_length,\n", "    dtype = dtype,\n", "    load_in_4bit = load_in_4bit,\n", "    # token = \"hf_...\", # use one if using gated models\n", ")"]}, {"cell_type": "markdown", "source": ["\n", "\n", "---\n", "\n"], "metadata": {"id": "B7NxXMhYTdh2"}}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "6bZsfBuZDeCL", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1713733488000, "user_tz": -120, "elapsed": 1879, "user": {"displayName": "David Ondrej", "userId": "10143559659389860010"}}, "outputId": "ebd27673-5bba-4b67-8b41-b6d0536e66b9"}, "outputs": [{"output_type": "stream", "name": "stderr", "text": ["Unsloth 2024.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"]}], "source": ["model = FastLanguageModel.get_peft_model(\n", "    model,\n", "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n", "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n", "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n", "    lora_alpha = 16,\n", "    lora_dropout = 0,\n", "    bias = \"none\",\n", "    use_gradient_checkpointing = \"unsloth\",\n", "    random_state = 3407,\n", "    use_rslora = False,\n", "    loftq_config = None,\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "LjY75GoYUCB8", "colab": {"base_uri": "https://localhost:8080/", "height": 145, "referenced_widgets": ["5f75f4ad407d4ee99506247a126e395e", "03afe290799c4c568e9fe314ced32fe9", "02013866a44a415bbde01b5b19d19c21", "c3d62ff4699d4b0c9ae89f57f4a98b31", "e84ad484590d4caa8c48c158f12037fb", "593c80a4755a4a78a8431ac1e52a3dfc", "7bb6f1f2065e435884ca35ae0d27237d", "b177d963d5d04a659ea03eea0d4bc42d", "2d29479e93a54aa1b9ae148944d112ae", "ef020d3703db44f6b87a34853f22be64", "6368ac0672a648f3a3b8404ef315395e", "c9535358e1fe496fa490629f3871600d", "744f0e0920e74d74960c939c2d7f5a7c", "35b7fe23cc2d4a49b00481dafeb6f5ed", "892ec7f7693e46f7b26883c5d1f22e9c", "30b8f644232145b3a32cd222d62fe7a7", "b79652204b5b4c16bddd926cb9ed1272", "3fecfc6542794baba672373486570d5b", "43e9e5fe3995440dadfee7c83c879451", "0de8223edee3420998ea3f6a6b0b5633", "7cd7e6bc7a3f41ddbe178420b73dd05d", "91a0f843386d44dc9a3a695668ba62e7", "7fa5d41ee469486fac0057ac17ff733a", "3aee53a413224424b651cbb8f9c43d47", "43b3168efe364334891ec3fae11a8580", "82c3d25ac8df449fb48cd8b978bb81db", "60ed6a634f9c417e8c9819a8f9972e61", "26f00ecaeb8c440ca78b544bdf5e0f34", "30a640d740ce4afa9129a45da0eea160", "8ee785d07d574381b0b2498d007576f4", "c975f3d5c9d34cfcaf711a50a8b77d20", "4905ffdb7f3c45db8726098876fd24ef", "e2900ece526341d8b659c0e12b922019", "168f09d8781c4c16844ba33785575b2a", "03c081694ce443d8aba94fe1eae0a52d", "68a7f6d8b24c4a0b8ab432f6599d5796", "34e595e225ec4c6290f79e5361386a31", "a9d8d618909e404884076d71cfc3a6d7", "f50266902ffc4c1f917d285f6456db5e", "dea8d078debf4ee2ad72763ab94e0959", "45812c220eb44aad82732b4c5f21146f", "4ac3c7536d764b66bf2c01d2d858c4c9", "b7a8cc2601834053ae6160ed3a2ce904", "df4d7329078b434ba3ac1cd5521125e4"]}, "executionInfo": {"status": "ok", "timestamp": 1713733496156, "user_tz": -120, "elapsed": 8158, "user": {"displayName": "David Ondrej", "userId": "10143559659389860010"}}, "outputId": "97b32f06-0ee0-47b9-e261-69aaab515b42"}, "outputs": [{"output_type": "display_data", "data": {"text/plain": ["Downloading readme:   0%|          | 0.00/11.6k [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "5f75f4ad407d4ee99506247a126e395e"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["Downloading data:   0%|          | 0.00/44.3M [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c9535358e1fe496fa490629f3871600d"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["Generating train split:   0%|          | 0/51760 [00:00<?, ? examples/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "7fa5d41ee469486fac0057ac17ff733a"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["Map:   0%|          | 0/51760 [00:00<?, ? examples/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "168f09d8781c4c16844ba33785575b2a"}}, "metadata": {}}], "source": ["# this is basically the system prompt\n", "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n", "\n", "### Instruction:\n", "{}\n", "\n", "### Input:\n", "{}\n", "\n", "### Response:\n", "{}\"\"\"\n", "\n", "EOS_TOKEN = tokenizer.eos_token\n", "def formatting_prompts_func(examples):\n", "    instructions = examples[\"instruction\"]\n", "    inputs       = examples[\"input\"]\n", "    outputs      = examples[\"output\"]\n", "    texts = []\n", "    for instruction, input, output in zip(instructions, inputs, outputs):\n", "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n", "        texts.append(text)\n", "    return { \"text\" : texts, }\n", "pass\n", "\n", "from datasets import load_dataset\n", "dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n", "dataset = dataset.map(formatting_prompts_func, batched = True,)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "95_Nn-89DhsL", "colab": {"base_uri": "https://localhost:8080/", "height": 104, "referenced_widgets": ["03fd5f663d4d4595b26a1080b0694083", "63f4a7ce688b448e80f4f842d47ad708", "20af0cc86ab74d808a322f8c5749f6d0", "ae51f9bbad444299b3a7c601477bbf7d", "23549bafc3244c7db7efd66f44a8a080", "4166bafd111c4271ae78b0ba2ac6fd92", "5fe19def47544bf4bbd381d5f9777c7c", "f06a3ff45761463fa60a29751a426d7a", "1787b73bf44f4eb6a94f92f9b59cc6d9", "0e514dede4514f79ac04ef438a6f9d01", "746559183dff4539aac306a8efb8a3e7"]}, "executionInfo": {"status": "ok", "timestamp": 1713733541179, "user_tz": -120, "elapsed": 45031, "user": {"displayName": "David Ondrej", "userId": "10143559659389860010"}}, "outputId": "bec72c3a-b268-4037-9e29-d03e5a7c81e9"}, "outputs": [{"output_type": "stream", "name": "stderr", "text": ["/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n", "  self.pid = os.fork()\n"]}, {"output_type": "display_data", "data": {"text/plain": ["Map (num_proc=2):   0%|          | 0/51760 [00:00<?, ? examples/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "03fd5f663d4d4595b26a1080b0694083"}}, "metadata": {}}], "source": ["from trl import SFTTrainer\n", "from transformers import TrainingArguments\n", "\n", "trainer = SFTTrainer(\n", "    model = model,\n", "    tokenizer = tokenizer,\n", "    train_dataset = dataset,\n", "    dataset_text_field = \"text\",\n", "    max_seq_length = max_seq_length,\n", "    dataset_num_proc = 2,\n", "    packing = False, # Can make training 5x faster for short sequences.\n", "    args = TrainingArguments(\n", "        per_device_train_batch_size = 2,\n", "        gradient_accumulation_steps = 4,\n", "        warmup_steps = 5,\n", "        max_steps = None, # increase this to make the model learn \"better\"\n", "        num_train_epochs=4\n", "        learning_rate = 2e-4,\n", "        fp16 = not torch.cuda.is_bf16_supported(),\n", "        bf16 = torch.cuda.is_bf16_supported(),\n", "        logging_steps = 1,\n", "        optim = \"adamw_8bit\",\n", "        weight_decay = 0.01,\n", "        lr_scheduler_type = \"linear\",\n", "        seed = 3407,\n", "        output_dir = \"outputs\",\n", "    ),\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "yqxqAZ7KJ4oL", "colab": {"base_uri": "https://localhost:8080/", "height": 1000}, "outputId": "6452cf4d-cf2d-4294-b59e-b731fa153328", "executionInfo": {"status": "ok", "timestamp": 1713734017724, "user_tz": -120, "elapsed": 476547, "user": {"displayName": "David Ondrej", "userId": "10143559659389860010"}}}, "outputs": [{"output_type": "stream", "name": "stderr", "text": ["==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n", "   \\\\   /|    Num examples = 51,760 | Num Epochs = 1\n", "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n", "\\        /    Total batch size = 8 | Total steps = 60\n", " \"-____-\"     Number of trainable parameters = 41,943,040\n"]}, {"output_type": "display_data", "data": {"text/plain": ["<IPython.core.display.HTML object>"], "text/html": ["\n", "    <div>\n", "      \n", "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n", "      [60/60 07:41, Epoch 0/1]\n", "    </div>\n", "    <table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", " <tr style=\"text-align: left;\">\n", "      <th>Step</th>\n", "      <th>Training Loss</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>1</td>\n", "      <td>1.814600</td>\n", "    </tr>\n", "    <tr>\n", "      <td>2</td>\n", "      <td>2.293200</td>\n", "    </tr>\n", "    <tr>\n", "      <td>3</td>\n", "      <td>1.689500</td>\n", "    </tr>\n", "    <tr>\n", "      <td>4</td>\n", "      <td>1.952400</td>\n", "    </tr>\n", "    <tr>\n", "      <td>5</td>\n", "      <td>1.645700</td>\n", "    </tr>\n", "    <tr>\n", "      <td>6</td>\n", "      <td>1.639900</td>\n", "    </tr>\n", "    <tr>\n", "      <td>7</td>\n", "      <td>1.217700</td>\n", "    </tr>\n", "    <tr>\n", "      <td>8</td>\n", "      <td>1.246900</td>\n", "    </tr>\n", "    <tr>\n", "      <td>9</td>\n", "      <td>1.069300</td>\n", "    </tr>\n", "    <tr>\n", "      <td>10</td>\n", "      <td>1.173900</td>\n", "    </tr>\n", "    <tr>\n", "      <td>11</td>\n", "      <td>0.963200</td>\n", "    </tr>\n", "    <tr>\n", "      <td>12</td>\n", "      <td>0.991000</td>\n", "    </tr>\n", "    <tr>\n", "      <td>13</td>\n", "      <td>0.929300</td>\n", "    </tr>\n", "    <tr>\n", "      <td>14</td>\n", "      <td>1.050700</td>\n", "    </tr>\n", "    <tr>\n", "      <td>15</td>\n", "      <td>0.883800</td>\n", "    </tr>\n", "    <tr>\n", "      <td>16</td>\n", "      <td>0.877000</td>\n", "    </tr>\n", "    <tr>\n", "      <td>17</td>\n", "      <td>1.005900</td>\n", "    </tr>\n", "    <tr>\n", "      <td>18</td>\n", "      <td>1.264200</td>\n", "    </tr>\n", "    <tr>\n", "      <td>19</td>\n", "      <td>0.988400</td>\n", "    </tr>\n", "    <tr>\n", "      <td>20</td>\n", "      <td>0.885500</td>\n", "    </tr>\n", "    <tr>\n", "      <td>21</td>\n", "      <td>0.921300</td>\n", "    </tr>\n", "    <tr>\n", "      <td>22</td>\n", "      <td>0.991900</td>\n", "    </tr>\n", "    <tr>\n", "      <td>23</td>\n", "      <td>0.859300</td>\n", "    </tr>\n", "    <tr>\n", "      <td>24</td>\n", "      <td>0.983100</td>\n", "    </tr>\n", "    <tr>\n", "      <td>25</td>\n", "      <td>1.070700</td>\n", "    </tr>\n", "    <tr>\n", "      <td>26</td>\n", "      <td>1.012700</td>\n", "    </tr>\n", "    <tr>\n", "      <td>27</td>\n", "      <td>1.049800</td>\n", "    </tr>\n", "    <tr>\n", "      <td>28</td>\n", "      <td>0.869200</td>\n", "    </tr>\n", "    <tr>\n", "      <td>29</td>\n", "      <td>0.843300</td>\n", "    </tr>\n", "    <tr>\n", "      <td>30</td>\n", "      <td>0.903300</td>\n", "    </tr>\n", "    <tr>\n", "      <td>31</td>\n", "      <td>0.866200</td>\n", "    </tr>\n", "    <tr>\n", "      <td>32</td>\n", "      <td>0.873700</td>\n", "    </tr>\n", "    <tr>\n", "      <td>33</td>\n", "      <td>0.998200</td>\n", "    </tr>\n", "    <tr>\n", "      <td>34</td>\n", "      <td>0.863200</td>\n", "    </tr>\n", "    <tr>\n", "      <td>35</td>\n", "      <td>0.966000</td>\n", "    </tr>\n", "    <tr>\n", "      <td>36</td>\n", "      <td>0.865400</td>\n", "    </tr>\n", "    <tr>\n", "      <td>37</td>\n", "      <td>0.891200</td>\n", "    </tr>\n", "    <tr>\n", "      <td>38</td>\n", "      <td>0.772900</td>\n", "    </tr>\n", "    <tr>\n", "      <td>39</td>\n", "      <td>1.096600</td>\n", "    </tr>\n", "    <tr>\n", "      <td>40</td>\n", "      <td>1.171800</td>\n", "    </tr>\n", "    <tr>\n", "      <td>41</td>\n", "      <td>0.911100</td>\n", "    </tr>\n", "    <tr>\n", "      <td>42</td>\n", "      <td>0.992800</td>\n", "    </tr>\n", "    <tr>\n", "      <td>43</td>\n", "      <td>0.960500</td>\n", "    </tr>\n", "    <tr>\n", "      <td>44</td>\n", "      <td>0.911200</td>\n", "    </tr>\n", "    <tr>\n", "      <td>45</td>\n", "      <td>0.932100</td>\n", "    </tr>\n", "    <tr>\n", "      <td>46</td>\n", "      <td>1.000200</td>\n", "    </tr>\n", "    <tr>\n", "      <td>47</td>\n", "      <td>0.876300</td>\n", "    </tr>\n", "    <tr>\n", "      <td>48</td>\n", "      <td>1.225000</td>\n", "    </tr>\n", "    <tr>\n", "      <td>49</td>\n", "      <td>0.923600</td>\n", "    </tr>\n", "    <tr>\n", "      <td>50</td>\n", "      <td>1.049000</td>\n", "    </tr>\n", "    <tr>\n", "      <td>51</td>\n", "      <td>1.033300</td>\n", "    </tr>\n", "    <tr>\n", "      <td>52</td>\n", "      <td>0.939700</td>\n", "    </tr>\n", "    <tr>\n", "      <td>53</td>\n", "      <td>1.017100</td>\n", "    </tr>\n", "    <tr>\n", "      <td>54</td>\n", "      <td>1.174200</td>\n", "    </tr>\n", "    <tr>\n", "      <td>55</td>\n", "      <td>0.801100</td>\n", "    </tr>\n", "    <tr>\n", "      <td>56</td>\n", "      <td>1.033900</td>\n", "    </tr>\n", "    <tr>\n", "      <td>57</td>\n", "      <td>0.889000</td>\n", "    </tr>\n", "    <tr>\n", "      <td>58</td>\n", "      <td>0.835600</td>\n", "    </tr>\n", "    <tr>\n", "      <td>59</td>\n", "      <td>0.868500</td>\n", "    </tr>\n", "    <tr>\n", "      <td>60</td>\n", "      <td>0.911300</td>\n", "    </tr>\n", "  </tbody>\n", "</table><p>"]}, "metadata": {}}], "source": ["# kicking off the actual training of the model\n", "trainer_stats = trainer.train()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "pCqnaKmlO1U9", "cellView": "form", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1713734017725, "user_tz": -120, "elapsed": 4, "user": {"displayName": "David Ondrej", "userId": "10143559659389860010"}}, "outputId": "4ef373b9-c7a9-4a51-e3e4-2baead446067"}, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["476.2261 seconds used for training.\n", "7.94 minutes used for training.\n", "Peak reserved memory = 8.982 GB.\n", "Peak reserved memory for training = 3.314 GB.\n", "Peak reserved memory % of max memory = 60.903 %.\n", "Peak reserved memory for training % of max memory = 22.471 %.\n"]}], "source": ["#@title Show final memory and time stats\n", "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n", "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n", "used_percentage = round(used_memory         /max_memory*100, 3)\n", "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n", "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n", "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n", "print(f\"Peak reserved memory = {used_memory} GB.\")\n", "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n", "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n", "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"]}, {"cell_type": "code", "source": ["FastLanguageModel.for_inference(model)\n", "inputs = tokenizer(\n", "[\n", "    alpaca_prompt.format(\n", "        \"List the prime numbers contained within the range.\", # instruction\n", "        \"1-50\", # input\n", "        \"\", # output - leave this blank for generation!\n", "    )\n", "], return_tensors = \"pt\").to(\"cuda\")\n", "\n", "outputs = model.generate(**inputs, max_new_tokens = 128, use_cache = True)\n", "tokenizer.batch_decode(outputs)"], "metadata": {"id": "kR3gIAX-SM2q", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1713735219832, "user_tz": -120, "elapsed": 4085, "user": {"displayName": "David Ondrej", "userId": "10143559659389860010"}}, "outputId": "9fe9d870-6f7d-4910-8c19-cc67ca7d9e92"}, "execution_count": null, "outputs": [{"output_type": "stream", "name": "stderr", "text": ["Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"]}, {"output_type": "execute_result", "data": {"text/plain": ["['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nList the prime numbers contained within the range.\\n\\n### Input:\\n1-50\\n\\n### Response:\\n2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47<|end_of_text|>']"]}, "metadata": {}, "execution_count": 16}]}, {"cell_type": "code", "source": ["FastLanguageModel.for_inference(model)\n", "inputs = tokenizer(\n", "[\n", "    alpaca_prompt.format(\n", "        \"Convert these binary numbers to decimal.\", # instruction\n", "        \"1010, 1101, 1111\", # input\n", "        \"\", # output - leave this blank for generation!\n", "    )\n", "], return_tensors = \"pt\").to(\"cuda\")\n", "\n", "from transformers import TextStreamer\n", "text_streamer = TextStreamer(tokenizer)\n", "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"], "metadata": {"id": "e2pEuRb1r2Vg", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1713735241234, "user_tz": -120, "elapsed": 2961, "user": {"displayName": "David Ondrej", "userId": "10143559659389860010"}}, "outputId": "6ddfa838-4443-49b6-b67a-e3fdeca20a46"}, "execution_count": null, "outputs": [{"output_type": "stream", "name": "stderr", "text": ["Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"]}, {"output_type": "stream", "name": "stdout", "text": ["Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n", "\n", "### Instruction:\n", "Convert these binary numbers to decimal.\n", "\n", "### Input:\n", "1010, 1101, 1111\n", "\n", "### Response:\n", "The decimal equivalent of 1010 is 10. The decimal equivalent of 1101 is 13. The decimal equivalent of 1111 is 15.<|end_of_text|>\n"]}]}, {"cell_type": "code", "source": ["model.save_pretrained(\"lora_model\") # Local saving\n", "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"], "metadata": {"id": "upcOlWe7A1vc"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["if False:\n", "    from unsloth import FastLanguageModel\n", "    model, tokenizer = FastLanguageModel.from_pretrained(\n", "        model_name = \"lora_model\", # THE MODEL YOU USED FOR TRAINING\n", "        max_seq_length = max_seq_length,\n", "        dtype = dtype,\n", "        load_in_4bit = load_in_4bit,\n", "    )\n", "    FastLanguageModel.for_inference(model)\n", "\n", "# alpaca_prompt = MUST run cells from above!\n", "\n", "inputs = tokenizer(\n", "[\n", "    alpaca_prompt.format(\n", "        \"What is a famous tall tower in Paris?\", # instruction\n", "        \"\", # input\n", "        \"\", # output - leave this blank for generation!\n", "    )\n", "], return_tensors = \"pt\").to(\"cuda\")\n", "\n", "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n", "tokenizer.batch_decode(outputs)"], "metadata": {"id": "MKX_XKs_BNZR", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1713734031149, "user_tz": -120, "elapsed": 4688, "user": {"displayName": "David Ondrej", "userId": "10143559659389860010"}}, "outputId": "e24a07f8-0cdf-441b-fe6c-025f9a4b6700"}, "execution_count": null, "outputs": [{"output_type": "stream", "name": "stderr", "text": ["Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"]}, {"output_type": "execute_result", "data": {"text/plain": ["[\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat is a famous tall tower in Paris?\\n\\n### Input:\\n\\n\\n### Response:\\nOne of the most famous tall towers in Paris is the Eiffel Tower. It is a wrought iron tower located on the Champ de Mars in Paris, France. It was built in 1889 as the entrance to the 1889 World's Fair, and it was designed by the French engineers Gustave Eiff\"]"]}, "metadata": {}, "execution_count": 12}]}, {"cell_type": "code", "source": ["if False:\n", "    # highly do NOT suggested - use Unsloth if possible\n", "    from peft import AutoPeftModelForCausalLM\n", "    from transformers import AutoTokenizer\n", "    model = AutoPeftModelForCausalLM.from_pretrained(\n", "        \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n", "        load_in_4bit = load_in_4bit,\n", "    )\n", "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"], "metadata": {"id": "yFfaXG0WsQuE"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["# Merge to 16bit\n", "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n", "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n", "\n", "# Merge to 4bit\n", "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n", "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n", "\n", "# Just LoRA adapters\n", "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n", "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"], "metadata": {"id": "iHjt_SMYsd3P"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["# Save to 8bit Q8_0\n", "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n", "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n", "\n", "# Save to 16bit GGUF\n", "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n", "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n", "\n", "# Save to q4_k_m GGUF\n", "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n", "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")"], "metadata": {"id": "FqfebeAdT073"}, "execution_count": null, "outputs": []}], "metadata": {"accelerator": "GPU", "colab": {"provenance": [{"file_id": "1efOx_rwZeF3i0YsirhM1xhYLtGNX6Fv3", "timestamp": 1754064715940}, {"file_id": "18KF6yNW9pRS7lwYk8zRLRI_otrpL84ev", "timestamp": 1713732901332}, {"file_id": "135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp", "timestamp": 1713724264856}, {"file_id": "10NbwlsRChbma1v55m8LAPYG15uQv6HLo", "timestamp": 1713459337061}, {"file_id": "1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_", "timestamp": 1708958229810}, {"file_id": "1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5", "timestamp": 1703608159823}, {"file_id": "1oW55fBmwzCOrBVX66RcpptL3a99qWBxb", "timestamp": 1702886138876}], "gpuType": "T4"}, "kernelspec": {"display_name": "Python 3", "name": "python3"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 0}